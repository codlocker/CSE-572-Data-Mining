{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8830f3f0",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fab29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc928e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    \n",
    "    def __init__(self, k) -> None:\n",
    "        self.k = k\n",
    "        self.centroids = None\n",
    "        self._sse_score = None\n",
    "\n",
    "    def euclidean_distance(self, data_point, centroids):\n",
    "        return np.sqrt(np.sum((centroids - data_point)**2, axis=1))\n",
    "\n",
    "    def __sum_of_squared_errors_calc(self, centroids, data, y):\n",
    "        sum_of_errors = 0.0\n",
    "        for idx, d in enumerate(data):\n",
    "            sum_of_errors += np.sum((centroids[y[idx]] - d) ** 2)\n",
    "\n",
    "        return sum_of_errors\n",
    "\n",
    "    def get_sum_of_squared_error(self):\n",
    "        return self._sse_score\n",
    "    \n",
    "    def fit(self, X, max_iterations=200):\n",
    "        self.centroids = np.random.uniform(\n",
    "            low=np.amin(X, axis=0),\n",
    "            high=np.amax(X, axis=0),\n",
    "            size=(self.k, X.shape[1]))\n",
    "\n",
    "        y = []\n",
    "        for _ in range(max_iterations):\n",
    "            y = []\n",
    "            for data_point in X:\n",
    "\n",
    "                distances = self.euclidean_distance(\n",
    "                    data_point=data_point,\n",
    "                    centroids=self.centroids)\n",
    "                # print(distances.shape)\n",
    "                cluster_num = np.argmin(distances)\n",
    "                y.append(cluster_num)\n",
    "            y = np.asarray(y)\n",
    "\n",
    "            cluster_indices = []\n",
    "\n",
    "            for idx in range(self.k):\n",
    "                cluster_indices.append(np.argwhere(y == idx))\n",
    "\n",
    "            cluster_centers = []\n",
    "\n",
    "            for i, indices in enumerate(cluster_indices):\n",
    "                if len(indices) == 0:\n",
    "                    cluster_centers.append(self.centroids[i])\n",
    "                else:\n",
    "                    cluster_centers.append(np.mean(X[indices], axis=0)[0])\n",
    "\n",
    "            if np.max(self.centroids - np.array(cluster_centers)) < 1e-3:\n",
    "                break\n",
    "            else:\n",
    "                self.centroids = np.array(cluster_centers)\n",
    "\n",
    "        # Calculate the final SSE after performing K-means\n",
    "        self._sse_score = self.__sum_of_squared_errors_calc(X, self.centroids, y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401024a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  (10000, 784)\n",
      "Labels:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(pd.read_csv('datasets/kmeans_data/data.csv', header=None))\n",
    "labels = np.ravel(pd.read_csv('datasets/kmeans_data/label.csv', header=None))\n",
    "print('Data: ', data.shape)\n",
    "print('Labels: ', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fa1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "no_of_clusters = unique_labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780ff701",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_kmeans_m = KMeansClustering(k=no_of_clusters)\n",
    "euclidean_kmeans_m_labels = euclidean_kmeans_m.fit(X=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "449b1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances = pairwise_distances(data, metric='cosine')\n",
    "cosine_kmeans_m = KMeansClustering(k=no_of_clusters)\n",
    "cosine_kmeans_m_labels = cosine_kmeans_m.fit(cosine_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec7cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_distances = pairwise_distances(data, metric='hamming')\n",
    "jaccard_kmeans_m = KMeansClustering(k=no_of_clusters)\n",
    "jaccard_kmeans_m_labels = jaccard_kmeans_m.fit(X=jaccard_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934554fb",
   "metadata": {},
   "source": [
    "#### Q1. Run K-means clustering with Euclidean, Cosine and Jarcard similarity. Specify K= the number of categorical values of y (the number of classifications). Compare the SSEs of Euclidean-K-means, Cosine-K-means, Jarcard-K-means. Which method is better?\n",
    "\n",
    "From the SSE values, the one with the lowest SSE is Jaccard-K-means. So using Jaccard K-Means is betterwith the lowest SSE of 1059.83."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8aab60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36425431.45791967"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse_euclidean_m = euclidean_kmeans_m.get_sum_of_squared_error()\n",
    "sse_euclidean_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692a4da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4943.109414510786"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse_cosine_m = cosine_kmeans_m.get_sum_of_squared_error()\n",
    "sse_cosine_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9950e6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008.0470855781577"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "see_jaccard_m = jaccard_kmeans_m.get_sum_of_squared_error()\n",
    "see_jaccard_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de4a35",
   "metadata": {},
   "source": [
    "#### Q2. Compare the accuracies of Euclidean-K-means Cosine-K-means, Jarcard-K-means. First, label each cluster using the majority vote label of the data points in that cluster. Later, compute the predictive accuracy of Euclidean-K-means, Cosine-K-means, Jarcard-K-means. Which metric is better? (10 points)\n",
    "\n",
    "Based on the accuracy computation of majority vote, Euclidean accuracy seems to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f30e322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Euclidean-K-means: 0.6046\n",
      "Accuracy Cosine-K-means: 0.4354\n",
      "Accuracy Jaccard-K-means: 0.2017\n"
     ]
    }
   ],
   "source": [
    "def label_clusters(labels, true_labels):\n",
    "    unique_labels = np.unique(true_labels)\n",
    "    cluster_labels = np.zeros(len(labels), dtype=np.int)\n",
    "    for cluster in range(no_of_clusters):\n",
    "        cluster_indices = np.where(labels == cluster)[0]\n",
    "        cluster_true_labels = true_labels[cluster_indices]\n",
    "        majority_label = np.argmax([np.sum(cluster_true_labels == label) for label in unique_labels])\n",
    "        cluster_labels[cluster_indices] = majority_label\n",
    "    return cluster_labels\n",
    "\n",
    "# Label clusters using majority vote\n",
    "cluster_labels_euclidean = label_clusters(euclidean_kmeans_m_labels, labels)\n",
    "cluster_labels_cosine = label_clusters(cosine_kmeans_m_labels, labels)\n",
    "cluster_labels_jaccard = label_clusters(jaccard_kmeans_m_labels, labels)\n",
    "\n",
    "# Compute predictive accuracy\n",
    "accuracy_euclidean = accuracy_score(labels, cluster_labels_euclidean)\n",
    "accuracy_cosine = accuracy_score(labels, cluster_labels_cosine)\n",
    "accuracy_jaccard = accuracy_score(labels, cluster_labels_jaccard)\n",
    "\n",
    "print(\"Accuracy Euclidean-K-means:\", accuracy_euclidean)\n",
    "print(\"Accuracy Cosine-K-means:\", accuracy_cosine)\n",
    "print(\"Accuracy Jaccard-K-means:\", accuracy_jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a926460",
   "metadata": {},
   "source": [
    "#### Q3: Set up the same stop criteria: “when there is no change in centroid position OR when the SSE value increases in the next iteration OR when the maximum preset value (e.g., 500, you can set the preset value by yourself) of iteration is complete”, for Euclidean-K-means, Cosine-K\u0002means, Jarcard-K-means. Which method requires more iterations and times to converge? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d820b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
